{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a5f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82a9661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\Downloads\\\\Projects\\\\Fraud Detection\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd99a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7234d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\Downloads\\\\Projects\\\\Fraud Detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acf7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "      root_dir: Path\n",
    "      train_path: Path\n",
    "      test_path:Path\n",
    "      train_data: Path\n",
    "      test_data:Path\n",
    "      preprocessor: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79506c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fraud_detection.utils.common import create_directories, read_yaml\n",
    "from fraud_detection.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdac3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "        root_dir = config.root_dir,\n",
    "        train_path = config.train_path,\n",
    "        test_path = config.test_path,\n",
    "        train_data = config.train_data,\n",
    "        test_data = config.test_data,\n",
    "        preprocessor = config.preprocessor\n",
    "        )\n",
    "\n",
    "        return data_transformation_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f092ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from fraud_detection.utils.common import save_object \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9de8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Placeholder utility functions (ensure you have these implemented elsewhere)\n",
    "def load_and_clean_data(file_path):\n",
    "    # Implement actual data loading and cleaning\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "def save_object(file_path, obj):\n",
    "    # Implement object serialization (e.g., using joblib or pickle)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def build_preprocessor(self, df: pd.DataFrame) -> ColumnTransformer:\n",
    "      \n",
    "      \"\"\"\n",
    "      Constructs a preprocessing pipeline for numerical and categorical features.\n",
    "\n",
    "      Parameters:\n",
    "      - df (pd.DataFrame): Input DataFrame (excluding target column 'Is Fraudulent').\n",
    "\n",
    "      Returns:\n",
    "      - ColumnTransformer: A scikit-learn transformer for preprocessing.\n",
    "      \"\"\"\n",
    "      # Drop the target column if present\n",
    "      df = df.drop(columns=[\"Is Fraudulent\"], errors=\"ignore\")\n",
    "\n",
    "      # Identify numerical and categorical columns\n",
    "      numeric_features = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "      categorical_features = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "  \n",
    "       # Pipelines with imputation\n",
    "      numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "      categorical_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "         ])\n",
    "\n",
    "    # ColumnTransformer to apply pipelines\n",
    "      preprocessor = ColumnTransformer(transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_features),\n",
    "        (\"cat\", categorical_pipeline, categorical_features)\n",
    "        ])\n",
    "\n",
    "      return preprocessor\n",
    "\n",
    "   \n",
    "\n",
    "    @staticmethod\n",
    "    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Adds engineered features to the dataframe.\"\"\"\n",
    "\n",
    "         # Convert to datetime, coercing errors to NaT\n",
    "        df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')\n",
    "    \n",
    "        print(\"Transaction Date column after conversion:\")\n",
    "        print(df['Transaction Date'].head())\n",
    "        print(\"Data type:\", df['Transaction Date'].dtype)\n",
    "    \n",
    "        # Transaction Amount Features\n",
    "        df['Log Transaction Amount'] = np.log1p(df['Transaction Amount'])\n",
    "        df['Amount Bin'] = pd.qcut(df['Transaction Amount'], q=4, labels=False)\n",
    "\n",
    "        # Time-based Features\n",
    "        df['Day of Week'] = df['Transaction Date'].dt.dayofweek\n",
    "        df['Is Weekend'] = df['Day of Week'].isin([5, 6]).astype(int)\n",
    "        df['Hour Bin'] = pd.cut(df['Transaction Hour'], bins=[0, 6, 12, 18, 24], labels=False)\n",
    "\n",
    "        # Address Discrepancy\n",
    "        df['Address Mismatch'] = (df['Shipping Address'] != df['Billing Address']).astype(int)\n",
    "\n",
    "        # Customer Behavior Features\n",
    "        df['Amount per Item'] = df['Transaction Amount'] / df['Quantity']\n",
    "        df['Age Amount Interaction'] = df['Customer Age'] * df['Transaction Amount']\n",
    "        df['Account Age Bin'] = pd.qcut(df['Account Age Days'], q=4, labels=False)\n",
    "\n",
    "        # Categorical Encoding\n",
    "        le = LabelEncoder()\n",
    "        categorical_cols = ['Payment Method', 'Product Category', 'Device Used', 'Customer Location']\n",
    "        for col in categorical_cols:\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "        # IP Address Features\n",
    "        df['IP First Octet'] = df['IP Address'].apply(lambda x: int(x.split('.')[0]) if isinstance(x, str) else 0)\n",
    "\n",
    "        # Fraud Risk Indicators\n",
    "        df['High Value Transaction'] = (df['Transaction Amount'] > df['Transaction Amount'].quantile(0.95)).astype(int)\n",
    "        df['New Account'] = (df['Account Age Days'] < 30).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_file(self, file_path: str):\n",
    "        \"\"\"Loads, cleans, engineers features, and returns feature/target split.\"\"\"\n",
    "        df = load_and_clean_data(file_path)\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        feature_columns = [\n",
    "            'Log Transaction Amount', 'Amount Bin', 'Day of Week', 'Is Weekend', 'Hour Bin',\n",
    "            'Address Mismatch', 'Amount per Item', 'Age Amount Interaction', 'Account Age Bin',\n",
    "            'Payment Method_encoded', 'Product Category_encoded', 'Device Used_encoded',\n",
    "            'Customer Location_encoded', 'IP First Octet', 'High Value Transaction', 'New Account'\n",
    "        ]\n",
    "        return df[feature_columns], df['Is Fraudulent']\n",
    "\n",
    "    def initiate_data_transformation_and_split(self):\n",
    "        \"\"\"Main function to load data, preprocess, and return train/test split.\"\"\"\n",
    "        # Load, clean, and engineer features\n",
    "        X_train, y_train = self.process_file(self.config.train_path)\n",
    "        X_test, y_test = self.process_file(self.config.test_path)\n",
    "\n",
    "        # Build and apply preprocessing pipeline\n",
    "        logging.info(\"Building preprocessing pipeline.\")\n",
    "        preprocessor = self.build_preprocessor(X_train)\n",
    "\n",
    "        logging.info(\"Applying preprocessing pipeline.\")\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "        # Save preprocessor\n",
    "        save_object(file_path=self.config.preprocessor, obj=preprocessor)\n",
    "\n",
    "         # Create validation set from training data\n",
    "        X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train_processed, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "\n",
    "        return X_train_final, X_val, X_test_processed, y_train_final, y_val, y_test, self.config.preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0666366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 22:35:04,112: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-30 22:35:04,115: INFO: common: created directory at: artifacts]\n",
      "[2025-06-30 22:35:04,116: INFO: common: created directory at: artifacts/data_transformation]\n",
      "Transaction Date column after conversion:\n",
      "0   2024-02-20 05:58:41\n",
      "1   2024-02-25 08:09:45\n",
      "2   2024-03-18 03:42:55\n",
      "3   2024-03-16 20:41:31\n",
      "4   2024-01-15 05:08:17\n",
      "Name: Transaction Date, dtype: datetime64[ns]\n",
      "Data type: datetime64[ns]\n",
      "Transaction Date column after conversion:\n",
      "0   2024-03-24 23:42:43\n",
      "1   2024-01-22 00:53:31\n",
      "2   2024-01-22 08:06:03\n",
      "3   2024-01-16 20:34:53\n",
      "4   2024-01-16 15:47:23\n",
      "Name: Transaction Date, dtype: datetime64[ns]\n",
      "Data type: datetime64[ns]\n",
      "[2025-06-30 22:35:04,381: INFO: 1234400622: Building preprocessing pipeline.]\n",
      "[2025-06-30 22:35:04,381: INFO: 1234400622: Applying preprocessing pipeline.]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.initiate_data_transformation_and_split()\n",
    "except Exception as e:\n",
    "    logging.exception(\"An error occurred during data transformation.\")\n",
    "    raise  # optional: re-raises the same error after logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
